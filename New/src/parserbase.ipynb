{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import zipfile\n",
    "from typing import Optional, Union, List, Callable, Dict, Tuple\n",
    "from dataclasses import dataclass, make_dataclass, field\n",
    "from abc import ABC\n",
    "\n",
    "PKG_ROOT = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "if not PKG_ROOT in sys.path:\n",
    "    sys.path.append(PKG_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDS:\n",
    "    \n",
    "    @classmethod\n",
    "    def make_names_old(cls, name:str, df:pd.DataFrame):\n",
    "        '''\n",
    "        Construct dataclass or nested dataclass from a dataframe\n",
    "        '''\n",
    "        if df.shape[1] == 2:\n",
    "            data = [(_[1], str, field(default=_[0])) for _ in df.values]\n",
    "            return make_dataclass(name, data)\n",
    "        else:\n",
    "            group = df.groupby(df.columns[0])\n",
    "            data = [(k, dataclass, field(default=cls.make_names_old(k, v.drop(v.columns[0], axis=1)))) for k, v in group]\n",
    "            return make_dataclass(name, data)\n",
    "\n",
    "    @classmethod\n",
    "    def make_names_(cls, name:str, df:pd.DataFrame):\n",
    "        '''\n",
    "        Construct dataclass or nested dataclass from a dataframe\n",
    "        '''\n",
    "        if df.shape[1] == 2:\n",
    "            data = [(_[1], str, field(default=_[0])) for _ in df.values]\n",
    "            return make_dataclass(name, data)\n",
    "        else:\n",
    "            group = df.groupby(df.columns[0])\n",
    "            if group.ngroups == 1:\n",
    "                return cls.make_names_(name, df.drop(df.columns[0], axis=1))\n",
    "            data = [(k, dataclass, field(default=cls.make_names_(k, v.drop(v.columns[0], axis=1)))) for k, v in group]\n",
    "            return make_dataclass(name, data)\n",
    "        \n",
    "    @classmethod\n",
    "    def make_names(cls, name:str, df:pd.DataFrame, ignoreCols:List=[]):\n",
    "        return cls.make_names_(name=name, df=df[[_ for _ in df.columns if not _ in ignoreCols]])\n",
    "\n",
    "    @staticmethod\n",
    "    def make_enum(name:str, df:pd.DataFrame, nameCol:int=1, valueCol:Optional[Union[int, list]]=None):\n",
    "        if df.shape[1] == 1:\n",
    "            df = df.reset_index()\n",
    "        nameCol = df.columns[nameCol]\n",
    "        if valueCol is None:\n",
    "            valueCol = df.columns.difference([nameCol])\n",
    "        else:\n",
    "            valueCol = df.columns[valueCol]\n",
    "        if len(valueCol) == 1:\n",
    "            valueCol = valueCol[0]\n",
    "        if isinstance(valueCol, (list, tuple, pd.Index)):\n",
    "            return Enum(name, df.set_index(nameCol)[valueCol].apply(tuple, axis=1).to_dict())\n",
    "        else:\n",
    "            return Enum(name, df.set_index(nameCol)[valueCol].to_dict())\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_map(df, cols:List):\n",
    "        d1 = df[df.columns[cols[:2]]].set_index(df.columns[cols[0]]).drop_duplicates().to_dict()\n",
    "        d2 = df[df.columns[cols[:2]]].set_index(df.columns[cols[1]]).drop_duplicates().to_dict()\n",
    "        d1.update(d2)\n",
    "        return d1\n",
    "    \n",
    "    @classmethod\n",
    "    def clean_duplicates(cls, df:pd.DataFrame, valueCol:int=-1, groupby:Optional[Union[List, str]]=None):\n",
    "        if groupby is None:\n",
    "            col = df.columns[valueCol]\n",
    "            df = df.groupby(col).apply(lambda x: x.reset_index())\n",
    "            if df.index.nlevels > 1:\n",
    "                df = df.droplevel(0)\n",
    "            df[col] = df.apply(lambda x: f'{re.sub(\"_+\", \"_\", x[col])}{(\"_\" + str(x.name)) if x.name > 0 else \"\"}', axis=1)\n",
    "            return df.reset_index(drop=True).drop('index', axis=1)\n",
    "        else:\n",
    "            return df.groupby(groupby).apply(lambda x:cls.clean_duplicates(x, valueCol=valueCol)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockBase:\n",
    "    \n",
    "    @staticmethod\n",
    "    def bytes2Num(barray:bytes, signed:bool=False, order:str='little'):\n",
    "        return int.from_bytes(barray, signed=signed, byteorder=order)\n",
    "    \n",
    "    @staticmethod\n",
    "    def num2Bytes(num:int, length:int, signed=False, order='little'):\n",
    "        return num.to_bytes(length, signed=signed, byteorder=order)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bytes2Str(barray:bytes):\n",
    "        ba = np.array(bytearray(barray))\n",
    "        p = np.argmax(np.logical_or(ba > 127\n",
    "                                   ,ba < 32))\n",
    "        if p == 0: ## All strings will have >1 length so when p == 0, it only means the string is the whole length of the bytearray\n",
    "            p = len(barray)\n",
    "        return barray[:p].decode('latin')\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_int(num:float):\n",
    "        '''\n",
    "        Integer will be coerced to float if np.nan exists in the same column\n",
    "        Force the value to int, if np.nan then return np.nan\n",
    "        '''\n",
    "        if isinstance(num, str): #0x0000 in dataframe\n",
    "            return int(num, 16)\n",
    "        if num == None:\n",
    "            return None\n",
    "        if np.isnan(num):\n",
    "            return None\n",
    "        return int(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueBlock(BlockBase):\n",
    "    \n",
    "    def __init__(self\n",
    "                 ,pbuffer: bytearray  #contents of parent segment\n",
    "                 ,name: str\n",
    "                 ,valueLoc: int\n",
    "                 ,size: int\n",
    "                 ,signed: bool = False\n",
    "                 ,order: str = 'little'\n",
    "                 ,optimize: Optional[int] = None\n",
    "                 ,skiphead: int = 0\n",
    "                 ,skiptail: int = 0):\n",
    "        self.pbuffer = pbuffer\n",
    "        self.name = name\n",
    "        self.valueLoc = self.to_int(valueLoc)\n",
    "        self.size = self.to_int(size)\n",
    "        self.signed = signed\n",
    "        self.order = order\n",
    "        self.optimize = self.to_int(optimize)\n",
    "        self.skiphead = skiphead\n",
    "        self.skiptail = skiptail\n",
    "        self.notVoid = self.not_void()\n",
    "    \n",
    "    def not_void(self):\n",
    "        if any([pd.isnull(self.valueLoc), pd.isnull(self.size)]):\n",
    "            return False\n",
    "        if any([self.pbuffer is None\n",
    "                ,len(self.pbuffer)==0\n",
    "                ,self.valueLoc < 0\n",
    "                ,self.size <= 0\n",
    "                ,self.valueLoc + self.size > len(self.pbuffer)]):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def bind_buffer(self, buffer:bytearray):\n",
    "        self.pbuffer = buffer\n",
    "        return self\n",
    "     \n",
    "    @property\n",
    "    def bytes(self):\n",
    "        if self.not_void:\n",
    "            return self.pbuffer[self.valueLoc+self.skiphead\n",
    "                               :self.valueLoc+self.size+self.skiphead-self.skiptail]\n",
    "        return None\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.not_void:\n",
    "            return self.bytes2Num(self.bytes, self.signed, self.order)\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def string(self):\n",
    "        if self.not_void:\n",
    "            return self.bytes2Str(self.bytes)\n",
    "        return None\n",
    "    \n",
    "    def set_value(self, value:Union[int, Enum]):            \n",
    "        if hasattr(value, 'value'):\n",
    "                value = value.value\n",
    "        value = self.to_int(value)\n",
    "        if self.notVoid and value is not None:\n",
    "            self.pbuffer[self.valueLoc+self.skiphead\n",
    "                         :self.valueLoc+self.size+self.skiphead-self.skiptail] = self.num2Bytes(value, self.size, self.signed, self.order) \n",
    "    \n",
    "            \n",
    "    def inc_value(self, inc_value:int=1):\n",
    "        if self.notVoid:\n",
    "            self.set_value(self.value + inc_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordBase(BlockBase):\n",
    "     \n",
    "    def __init__(self, segList:List, nameMap: Optional[Dict]=None, template:Optional[bytearray]=None):\n",
    "        '''\n",
    "        SegList has the format of [(name, length, datatype)]\n",
    "        datatype includes: name, which will be mapped to codes in nameMap\n",
    "                           int\n",
    "                           str\n",
    "                           None\n",
    "        '''\n",
    "        self.pattern = re.compile(b''.join([f'(?P<{n}>.{{{s}}})'.encode() for (n, s, t) in segList]))\n",
    "        self.names, self.sizes, self.dtypes = zip(*segList)\n",
    "        self.sizes = np.array(self.sizes)\n",
    "        self.sizeMap = dict(zip(self.names, self.sizes))\n",
    "        self.rangeMap = dict([(n, r) for (n, r) in zip(self.names, [range(*_) for _ in zip(np.concatenate([[0], self.sizes.cumsum()])\n",
    "                                                                                           ,self.sizes.cumsum())])])\n",
    "        self.typeMap = dict(zip(self.names, self.dtypes))\n",
    "        self.size = self.sizes.sum()\n",
    "        self.nameMap = nameMap\n",
    "        if template is None:\n",
    "            self.template = bytearray(b'0' * self.size)\n",
    "        else:\n",
    "            self.template = template\n",
    "            \n",
    "            \n",
    "    def parse(self, buffer:bytearray):\n",
    "        return pd.DataFrame(_.groupdict() for _ in self.pattern.finditer(buffer))\n",
    "    \n",
    "    def infer_col(self, col:pd.Series):\n",
    "        dtype = self.typeMap.get(col.name, None)\n",
    "        if dtype == None:\n",
    "            return col\n",
    "        if dtype == int:\n",
    "            return col.apply(self.bytes2Num)\n",
    "        if dtype == str:\n",
    "            return col.apply(self.bytes2Str)\n",
    "        if dtype.upper() == 'INTNAME':\n",
    "            return col.apply(lambda x: self.nameMap(self.bytes2Num(x)).name)\n",
    "        if dtype.upper() == 'STRNAME':\n",
    "            return col.apply(lambda x: self.nameMap(self.bytes2Str(x)).name)\n",
    "        \n",
    "        \n",
    "    def inverse_infer_col(self, col:pd.Series):\n",
    "        col = col.apply(lambda x: x.name if hasattr(x, 'name') else x)\n",
    "        dtype = self.typeMap.get(col.name, None)\n",
    "        size = self.sizeMap.get(col.name)\n",
    "        if dtype is None:\n",
    "            return col\n",
    "        if dtype == int:\n",
    "            return col.apply(lambda x: self.num2Bytes(x, size))\n",
    "        if dtype == str:\n",
    "            return col.apply(lambda x: x.encode().ljust(size, b'\\x00'))\n",
    "        if dtype.upper() == 'INTNAME':\n",
    "            return col.apply(lambda x: self.num2Bytes(getattr(self.nameMap, x).value, size))\n",
    "        if dtype.upper() == 'STRNAME':\n",
    "            return col.apply(lambda x: getattr(self.nameMap, x).value.encode().ljust(size, b'\\x00'))\n",
    "            \n",
    "    \n",
    "    def infer(self, df:pd.DataFrame):\n",
    "        return pd.concat([self.infer_col(col) for name, col in df.iteritems()], axis=1)\n",
    "    \n",
    "    \n",
    "    def make_records(self, values:pd.DataFrame, repeatCol:int=-1):\n",
    "        '''\n",
    "        @params values is a dataframe with the last column as the repeats of each row\n",
    "        ColNames has to match the segment names in the pattern\n",
    "        '''\n",
    "        df = values.groupby(values.columns[repeatCol]).apply(lambda x: pd.concat([x] * x.name)).reset_index(drop=True).drop(values.columns[repeatCol], axis=1)\n",
    "        df = pd.concat([self.inverse_infer_col(col) for name, col in df.iteritems()], axis=1)\n",
    "        temp_df = pd.DataFrame([self.pattern.search(self.template).groupdict()] * df.shape[0])\n",
    "        temp_df.update(df)\n",
    "        return temp_df\n",
    "\n",
    "    def make_recordIndex_from_dict(self, values:List[pd.Series], repeats:Optional[Union[int, Tuple, List, np.ndarray]]=1):\n",
    "        values = [pd.Series(v, name=k) for k, v in values.items()]\n",
    "        repeats = np.array(repeats).ravel()\n",
    "        if repeats.size == 1:\n",
    "            repeats = np.tile(repeats, values[0].size)\n",
    "        df = pd.DataFrame(values).T\n",
    "        df = df[[_ for _ in self.names if _ in df.columns]] # make sure the sequence is aligned with pattern segment sequence\n",
    "        df.insert(df.shape[1], 'REPEATS', repeats)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableBlock(ValueBlock):\n",
    "    '''\n",
    "    Create ValueBlock that using a table of values to fill\n",
    "    refTable includes Kit, Race, Class, Gender, Alignment and RacialEnemy defined in CRETABLE.csv file\n",
    "    '''\n",
    "    \n",
    "    def __init__(self\n",
    "                ,pbuffer: bytearray\n",
    "                ,name: str\n",
    "                ,valueLoc: int\n",
    "                ,size: int\n",
    "                ,signed: bool\n",
    "                ,order: str\n",
    "                ,refTableName: str\n",
    "                ,optimize: Optional[int]\n",
    "                ,refTable: Enum):\n",
    "        super().__init__(pbuffer, name, valueLoc, size, signed, order, optimize)\n",
    "        self.refTable = refTable\n",
    "        \n",
    "        \n",
    "    def inc_value(self, inc_value:int=1):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        v = super().value\n",
    "        if v is not None:\n",
    "            return self.refTable(v).name\n",
    "        return None\n",
    "    \n",
    "    def set_value(self, value:Union[int, Enum, str]):\n",
    "        if isinstance(value, str):\n",
    "            for k, v in self.refTable.__members__.items():\n",
    "                if value.upper() == k:\n",
    "                    value = v.value\n",
    "                    break\n",
    "            else:\n",
    "                value = None\n",
    "        super().set_value(value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegBlock(BlockBase):\n",
    "    '''\n",
    "    Base class of subStructures as defined in GAMSEGS.csv and CRESEGS.csv\n",
    "    Used for dummy subStructures that will not be modified\n",
    "    '''\n",
    "    \n",
    "    def __init__(self\n",
    "                ,savRef: object\n",
    "                ,parentRef: object\n",
    "                ,pbuffer: bytearray\n",
    "                ,name: str\n",
    "                ,countLoc: int  #offset pointer to buffer\n",
    "                ,offsetLoc: int  #count pointer to buffer\n",
    "                ,sizeLoc: int\n",
    "                ,sizeValue: Optional[int] = None\n",
    "                ,countValue: Optional[int] = None\n",
    "                ,offsetValue: Optional[int] = None):  #only the header segment has no offset value\n",
    "        self.savRef = savRef\n",
    "        self.parentRef = parentRef\n",
    "        self.resourceDir = savRef.resourceDir\n",
    "        self.name = name\n",
    "        self.pbuffer = pbuffer\n",
    "        self.sizeValue = self.to_int(sizeValue)\n",
    "        self.countValue = self.to_int(countValue)\n",
    "        self.offsetValue = self.to_int(offsetValue)\n",
    "        self.offsetBlock = ValueBlock(self.pbuffer, 'OFFSET', self.to_int(offsetLoc), 4)\n",
    "        self.countBlock = ValueBlock(self.pbuffer, 'COUNT', self.to_int(countLoc), 4)\n",
    "        self.sizeBlock = ValueBlock(self.pbuffer, 'SIZE', self.to_int(sizeLoc), 4)\n",
    "        self.buffer = self.pbuffer[self.offsetValue: self.offsetValue + self.sizeValue]\n",
    "        self.previous = None\n",
    "\n",
    "            \n",
    "    def pack(self):\n",
    "        self.sizeValue = len(self.buffer)\n",
    "        if self.previous is not None:\n",
    "            self.offsetValue = self.previous.offsetValue + self.previous.sizeValue\n",
    "        return self.buffer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordsBlock(SegBlock):\n",
    "    '''\n",
    "    Parse subStructures in CRE files that's composed of a list of records\n",
    "    including spell, item, effect, and item\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.countValue = self.countBlock.value\n",
    "        self.df = self.Pattern.parse(self.buffer)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        display(self.Pattern.infer(self.df))\n",
    "        return f'{self.__class__} {self.name}'\n",
    "    \n",
    "    def add_from_df(self, values:pd.DataFrame, repeatCol=-1, deduplicate_by:List=[]):\n",
    "        df = self.Pattern.make_records(values, repeatCol=repeatCol)\n",
    "        if deduplicate_by:\n",
    "            self.remove({k:v for k, v in df.drop_duplicates().items() if k in deduplicate_by})\n",
    "        self.df = pd.concat([self.df, df]).reset_index(drop=True)\n",
    "        self.post_op()\n",
    "        \n",
    "    \n",
    "    def add(self, values:Dict, repeats:Union[int, Tuple, List, np.ndarray]=1, deduplicate_by:List=[]):\n",
    "        '''\n",
    "        If duplicated records are not allowed - such as in the case of knownspells, use the deduplicate_by parameter to find existing records having same values in these columns\n",
    "        remove those record before adding new ones\n",
    "        '''\n",
    "        values = self.Pattern.make_recordIndex_from_dict(values, repeats)\n",
    "        self.add_from_df(values, deduplicate_by=deduplicate_by)\n",
    "        \n",
    "        \n",
    "    def remove(self, values:Dict, remove_n:Optional[Union[int, Tuple, List, np.ndarray]]=None):\n",
    "        '''\n",
    "        Remove the first n records that matchs the values, if set to None, remove all\n",
    "        '''\n",
    "        iMap = self.Pattern.make_recordIndex_from_dict(values, remove_n)  # sequnces of cols are ensured to be aligned with Pattern segments in the called function\n",
    "        group = self.df.groupby(iMap.columns[:-1].tolist())\n",
    "        iMap = iMap.set_index([_ for _ in iMap.columns if _ != 'REPEATS']).to_dict()['REPEATS']\n",
    "        self.df = group.apply(lambda x: x.head(0 if iMap.get(x.name, 0) is None else max(0, x.shape[0] - iMap.get(x.name, 0)))).reset_index(drop=True)\n",
    "        self.post_op()\n",
    "        \n",
    "        \n",
    "    def replace(self, values:pd.DataFrame):\n",
    "        self.df = self.Pattern.make_records(values)\n",
    "        self.post_op()\n",
    "        \n",
    "    \n",
    "    def post_op(self):\n",
    "        '''\n",
    "        Defines the actions after record df is modified, such as reset_index\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def pack(self):\n",
    "        self.countValue = self.df.shape[0]\n",
    "        return super().pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Party(SegBlock):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.CREIndexLength, self.CREOffsetLoc, self.CRESizeLoc = self.get_CRE_locs()\n",
    "        self.parse_party()\n",
    "        \n",
    "    def get_CRE_locs(self):\n",
    "        loc_df = self.savRef.creLocs\n",
    "        CREIndexLen = self.to_int(loc_df[loc_df['SubSegs']=='Header']['SizeValue'].iloc[0])\n",
    "        CREOffsetLoc, CRESizeLoc =  loc_df[loc_df['SubSegs']=='CRE'][['OffsetLoc', 'SizeLoc']].iloc[0].apply(self.to_int)\n",
    "        return CREIndexLen, CREOffsetLoc, CRESizeLoc\n",
    "    \n",
    "    def parse_party(self):\n",
    "        self.indexHeader = self.buffer[: self.CREIndexLength * self.countBlock.value]\n",
    "        self.CREOffsetBlocks = [ValueBlock(self.buffer, 'CREOFFSET', self.CREOffsetLoc + self.CREIndexLength * i, 4) for i in range(self.countBlock.value)]\n",
    "        self.CRESizeBlocks = [ValueBlock(self.buffer, 'CRESIZE', self.CRESizeLoc + self.CREIndexLength * i, 4) for i in range(self.countBlock.value)]\n",
    "        vRecords = self.savRef.creValues\n",
    "        tRecords = self.savRef.creTables\n",
    "        sRecords = self.savRef.creSegs\n",
    "        self.CRES = [CRE(self.savRef\n",
    "                        ,self\n",
    "                        ,self.pbuffer[o.value: o.value + s.value]\n",
    "                        ,vRecords\n",
    "                        ,tRecords\n",
    "                        ,sRecords\n",
    "                        ) for o, s in zip(self.CREOffsetBlocks, self.CRESizeBlocks)]\n",
    "        \n",
    "    def pack(self):\n",
    "        [_.pack() for _ in self.CRES]\n",
    "        self.buffer = bytearray(b''.join([self.indexHeader, *[_.buffer for _ in self.CRES]]))\n",
    "        self.sizeValue = len(self.buffer)\n",
    "        self.offsetValue = self.previous.offsetValue + self.previous.sizeValue\n",
    "        creSizes = np.array([_.size for _ in self.CRES])\n",
    "        creOffsets = np.concatenate([[0], creSizes]).cumsum()[:-1] + len(self.indexHeader) + self.offsetValue\n",
    "        for o, s, ob, sb in zip(creOffsets, creSizes, self.CREOffsetBlocks, self.CRESizeBlocks):\n",
    "            ob.bind_buffer(self.buffer).set_value(int(o))\n",
    "            sb.bind_buffer(self.buffer).set_value(int(s))\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Globals(RecordsBlock):\n",
    "    \n",
    "    Pattern = RecordBase(segList=[('NAME', 32, str)\n",
    "                                 ,('TYPE', 2, int)\n",
    "                                 ,('REF', 2, int)\n",
    "                                 ,('DWORD', 4, int)\n",
    "                                 ,('INT', 4, int)\n",
    "                                 ,('DOUBEL', 8, int)\n",
    "                                 ,('SCRIPT', 32, str)])\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnownSpells(RecordsBlock):\n",
    "    \n",
    "    Pattern = RecordBase(segList=[('SPELL', 8, 'STRNAME')\n",
    "                                 ,('LEVEL', 2, int)\n",
    "                                 ,('TYPE', 2, int)])\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.Pattern.nameMap = self.savRef.SPELL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellMemorization(RecordsBlock):\n",
    "    \n",
    "    Pattern = RecordBase(segList=[('LEVEL', 2, int)\n",
    "                                 ,('BASECOUNT', 2, int)\n",
    "                                 ,('EFFCOUNT', 2, int)\n",
    "                                 ,('TYPE', 2, int)\n",
    "                                 ,('INDEX', 4, int)\n",
    "                                 ,('COUNTMEM', 4, int)])\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "            \n",
    "    def post_op(self):\n",
    "        self.df.sort_values(['TYPE', 'LEVEL'], inplace=True)\n",
    "        \n",
    "    def update_mem(self, idf:pd.DataFrame):\n",
    "        idf = pd.concat([self.inverse_infer_col(col) for name, col in idf.iteritems()], axis=1)\n",
    "        self.df = pd.merge(self.df.drop('COUNTMEM', axis=1), idf, how='left', left_on=['LEVEL', 'TYPE'], right_on=['LEVEL', 'TYPE'])\n",
    "        self.df.fillna(value={'INDEX': self.df['INDEX'].ffill().bfill()\n",
    "                              ,'COUNTMEM': b'\\x00\\x00\\x00\\x00'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemorizedSpells(RecordsBlock):\n",
    "    \n",
    "    TypeMap = {b'SPWI': 1\n",
    "              ,b'SPCL': 3\n",
    "              ,b'SPIN': 2\n",
    "              ,b'SPPR': 0\n",
    "              ,b'SPSD': 4}\n",
    "    \n",
    "    Pattern = RecordBase(segList=[('SPELL', 8,'STRNAME')\n",
    "                                 ,('COUNTMEM', 4, int)])\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.Pattern.nameMap = self.savRef.SPELL\n",
    "        \n",
    "    def post_op(self):\n",
    "        self.df.insert(0, 'TYPE', self.df['SPELL'].apply(lambda x: self.TypeMap.get(x[:4])))\n",
    "        self.df.insert(0, 'LEVEL', self.df['SPELL'].apply(lambda x: int(x[4:5])))\n",
    "        self.df = self.df.sort_values(['TYPE', 'LEVEL'])\n",
    "        idf = self.df.groupby(['TYPE', 'LEVEL'])['COUNTMEM'].count().reset_index()\n",
    "        idf.insert(idf.shape[1] - 1, 'INDEX', idf['COUNTMEM'].shift(fill_value=0).cumsum())\n",
    "        self.df = self.df.drop(['TYPE', 'LEVEL'], axis=1)\n",
    "        self.parentRef.SpellMemorization.update_mem(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Effects(RecordsBlock):\n",
    "    Pattern = RecordBase(segList=[('NULL', 8, int)\n",
    "                                 ,('OPCODE', 4, 'INTNAME')\n",
    "                                 ,('TARGET', 4, int)\n",
    "                                 ,('POWER', 4, int)\n",
    "                                 ,('PARAM1', 4, int)\n",
    "                                 ,('PARAM2', 4, int)\n",
    "                                 ,('TIME', 4, int)\n",
    "                                 ,('OTHER', 232, None)]\n",
    "                         ,template=bytearray([0, 0, 0, 0, 0, 0, 0, 0, 233, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 95, \n",
    "                                              0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255, \n",
    "                                              255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                                              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.Pattern.nameMap = self.savRef.EFFECT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Items(RecordsBlock):\n",
    "    \n",
    "    Pattern = RecordBase(segList=[('NAME', 8, 'STRNAME')\n",
    "                                 ,('EXPIRE', 1, int)\n",
    "                                 ,('ELAPSED', 1, int)\n",
    "                                 ,('QUALITY1', 2, int)\n",
    "                                 ,('QUALITY2', 2, int)\n",
    "                                 ,('QUALITY3', 2, int)\n",
    "                                 ,('IDENTIFIED', 1, int)\n",
    "                                 ,('UNSTEALABLE', 1, int)\n",
    "                                 ,('STOLEN', 1, int)\n",
    "                                 ,('UNDROPPABLE', 1, int)]\n",
    "                         ,template=b'CLCK30\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00')\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.Pattern.nameMap = self.savRef.ITEM\n",
    "        \n",
    "    def post_op(self):\n",
    "        self.parentRef.ItemSlots.place(self.Pattern.infer_col(self.df['NAME']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSlots(SegBlock):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.slots = {k: v.value.copy() for k, v in getattr(self.savRef, self.parentRef.parentRef.name.upper() + 'SLOTS').__members__.items()}\n",
    "        \n",
    "    def place(self, itms:pd.Series):\n",
    "        fill = [-1] * (max([max(_) for _ in self.slots.values()]) + 1)\n",
    "        bins = [_ for _ in self.slots.keys() if _ != 'NONE']\n",
    "        for i, itm in enumerate(itms):\n",
    "            slot = getattr(self.savRef.ITEMCONTAINER, itm)\n",
    "            for cname in [slot, 'MISC', *bins]:\n",
    "                c = self.slots.get(cname)\n",
    "                if len(c) > 0:\n",
    "                    fill[c.pop()] = i\n",
    "                    break\n",
    "        self.buffer = bytearray(b''.join([self.num2Bytes(_, length=2, signed=True) for _ in fill]))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamCreBase(ABC, BlockBase):\n",
    "    \n",
    "    SEGMAP = {\"KnownSpells\": KnownSpells\n",
    "             ,\"SpellMemorization\": SpellMemorization\n",
    "             ,\"MemorizedSpells\": MemorizedSpells\n",
    "             ,\"Effects\": Effects\n",
    "             ,\"Items\": Items\n",
    "             ,\"ItemSlots\": ItemSlots\n",
    "             ,\"Party\": Party\n",
    "             ,\"NPC\": Party\n",
    "             ,\"Global\": Globals}\n",
    "        \n",
    "        \n",
    "    def init_seg_size(self):\n",
    "        df = self.segRecords.copy()\n",
    "        df.insert(df.shape[1]\n",
    "                 ,'OffsetValue'\n",
    "                 ,self.segRecords['OffsetLoc'].apply(lambda x: np.nan if pd.isnull(x) else self.bytes2Num(self.buffer[self.to_int(x)\n",
    "                                                                                                                     :self.to_int(x)+4])))\n",
    "        df1 = df[df['OffsetValue']!=0].fillna(value={'OffsetValue': 0})\n",
    "        df2 = df[df['OffsetValue']==0]\n",
    "        df1['SizeValue'] = df1['OffsetValue'].shift(-1, fill_value=self.size) - df1['OffsetValue']\n",
    "        self.segRecords = pd.concat([df1, df2]).fillna(value={'SizeValue': 0}).sort_index()\n",
    "        \n",
    "    \n",
    "    def init_values(self, buffer:Optional[bytearray]=None): \n",
    "        self.VALUES = pd.Series([], dtype=object)\n",
    "        if not self.valueRecords.empty:\n",
    "            if buffer == None:\n",
    "                buffer = self.SEGS[0].buffer\n",
    "            self.VALUES = self.valueRecords.apply(lambda x: ValueBlock(buffer, *x), axis=1)\n",
    "            for i in range(self.VALUES.size):\n",
    "                setattr(self, self.VALUES[i].name, self.VALUES[i])\n",
    "            \n",
    "            # self.valueRecords.apply(lambda x: setattr(self, x[0], ValueBlock(buffer, *x[1:])),axis=1)\n",
    "            # self.VALUES = make_dataclass('VALUES', self.valueRecords.apply(lambda x: (x[0]\n",
    "            #                                                                          ,ValueBlock\n",
    "            #                                                                          ,ValueBlock(buffer, *x[1:]))\n",
    "            #                                                                ,axis=1))\n",
    "            \n",
    "            \n",
    "    def init_tables(self, buffer:Optional[bytearray]=None):\n",
    "        self.TABLEVALUES = pd.Series([], dtype=object)\n",
    "        if not self.tableRecords.empty:\n",
    "            if buffer == None:\n",
    "                buffer = self.buffer\n",
    "            self.TABLEVALUES = self.tableRecords.apply(lambda x: TableBlock(buffer, *x, getattr(self.savRef, x['RefTable'])) ,axis=1)\n",
    "            for i in range(self.TABLEVALUES.size):\n",
    "                setattr(self, self.TABLEVALUES[i].name, self.TABLEVALUES[i])\n",
    "            # self.TABLES = make_dataclass('TABLES', self.tableRecords.apply(lambda x: (x[0]\n",
    "            #                                                                          ,TableBlock\n",
    "            #                                                                          ,TableBlock(buffer\n",
    "            #                                                                                      ,*x[1:]\n",
    "            #                                                                                      ,getattr(self.savRef, x['RefTable'])))\n",
    "            #                                                                ,axis=1))\n",
    "                                                                           \n",
    "            \n",
    "    def init_segs(self, buffer:Optional[bytearray]=None):\n",
    "        '''\n",
    "        Need to keep sequence order\n",
    "        So use pd.Series instead of dataclass\n",
    "        '''\n",
    "        self.SEGS = pd.Series([], dtype=object)\n",
    "        if not self.segRecords.empty:\n",
    "            if buffer == None:\n",
    "                buffer = self.buffer\n",
    "            self.SEGS = (self.segRecords\n",
    "                         .set_index(self.segRecords.columns[0])\n",
    "                         .apply(lambda x: self.SEGMAP.get(x.name, SegBlock)(self.savRef, self, buffer, x.name, *x), axis=1))\n",
    "            for i in range(1, self.SEGS.size):\n",
    "                self.SEGS[i].previous = self.SEGS[i-1]\n",
    "                setattr(self, self.SEGS[i].name, self.SEGS[i])\n",
    "            \n",
    "            \n",
    "    def pack(self):\n",
    "        self.buffer = bytearray(b''.join([_.pack() for _ in self.SEGS]))\n",
    "        self.size = len(self.buffer)\n",
    "        for seg in self.SEGS:\n",
    "            seg.offsetBlock.bind_buffer(self.buffer).set_value(seg.offsetValue)\n",
    "            seg.countBlock.bind_buffer(self.buffer).set_value(seg.countValue)\n",
    "            seg.sizeBlock.bind_buffer(self.buffer).set_value(seg.sizeValue)\n",
    "        return self.buffer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAM(GamCreBase):\n",
    "    \n",
    "    def __init__(self\n",
    "                 ,savRef: object\n",
    "                 ,buffer: bytearray):\n",
    "        self.savRef = savRef\n",
    "        self.resourceDir = savRef.resourceDir\n",
    "        self.buffer = buffer\n",
    "        self.size = len(self.buffer)\n",
    "        self.valueRecords = savRef.gamValues\n",
    "        self.segRecords = savRef.gamSegs\n",
    "        self.VALUES = make_dataclass('SEGS', [])\n",
    "        self.SEGS = pd.Series([], dtype=object)\n",
    "        self.init_seg_size()\n",
    "        self.init_segs()\n",
    "        self.init_values()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRE(GamCreBase):\n",
    "    \n",
    "    def __init__(self\n",
    "                ,savRef:object\n",
    "                ,parentRef:object\n",
    "                ,buffer: bytearray\n",
    "                ,valueRecords: Optional[pd.DataFrame] = None\n",
    "                ,tableRecords: Optional[pd.DataFrame] = None\n",
    "                ,segRecords: Optional[pd.DataFrame] = None\n",
    "                ):\n",
    "        self.savRef = savRef\n",
    "        self.parentRef = parentRef\n",
    "        self.buffer = buffer\n",
    "        self.size = len(self.buffer)\n",
    "        if valueRecords is None:\n",
    "            self.valueRecords = pd.read_csv(os.path.join(self.savRef.resourceDir, 'CREVALUES.csv'), index_col=0)\n",
    "        else:\n",
    "            self.valueRecords = valueRecords.copy()\n",
    "        if tableRecords is None:\n",
    "            self.tableRecords = pd.read_csv(os.path.join(self.savRef.resourceDir, 'CRETABLES.csv'), index_col=0)\n",
    "        else:\n",
    "            self.tableRecords = tableRecords.copy()\n",
    "        if segRecords is None:\n",
    "            self.segRecords = pd.read_csv(os.path.join(self.savRef.resourceDir, 'CRESEGS.csv'), index_col=0)\n",
    "        else:\n",
    "            self.segRecords = segRecords.copy()\n",
    "        self.VALUES = make_dataclass('SEGS', [])\n",
    "        self.TABLES = make_dataclass('SEGS', [])\n",
    "        self.SEGS = pd.Series([], dtype=object)\n",
    "        self.init_seg_size()\n",
    "        self.init_segs() \n",
    "        self.init_values()\n",
    "        self.init_tables()\n",
    "             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sav(GamCreBase):\n",
    "    \n",
    "    def __init__(self, savefile:str, game:str='BGEE'):   # for other games, RACE, KIT, STATES, CLASS may need to copy from BGEE since the list is more comprehensive\n",
    "        self.savefile = os.path.join(PKG_ROOT, 'saves', 'original', savefile)\n",
    "        self.modified = os.path.join(PKG_ROOT, 'saves', 'modified', 'Edited_'+savefile)\n",
    "        self.resourceDir = os.path.join(PKG_ROOT, 'resources', game)\n",
    "        self.zipfile = zipfile.ZipFile(self.savefile)\n",
    "        self.filelist = self.zipfile.namelist()\n",
    "        self.files = [self.zipfile.read(_) for _ in self.filelist]\n",
    "        self.gamStr = self.files[1].decode('latin')\n",
    "        self.gamBuffer = bytearray(self.files[1])\n",
    "        self.gamVersion = re.findall(r'GAME\\s*V\\d+\\.\\d+', self.gamStr)[0]  #always 'GAMEV2.0'\n",
    "        self.creVersion = re.findall(r'CRE\\s*V\\d+\\.\\d+', self.gamStr)[0]  #always 'CRE V1.0'\n",
    "        self.load_dfs()\n",
    "        self.make_names()\n",
    "        self.GAM = GAM(savRef=self, buffer=self.gamBuffer)\n",
    "        self.Party = self.GAM.SEGS.Party.CRES\n",
    "        self.NPC = self.GAM.SEGS.NPC.CRES\n",
    "        \n",
    "        \n",
    "    def load_dfs(self):\n",
    "        self.gamValues = pd.read_csv(os.path.join(self.resourceDir, 'GAMVALUES.csv'), index_col=0)\n",
    "        self.gamSegs = pd.read_csv(os.path.join(self.resourceDir, 'GAMSEGS.csv'), index_col=0)\n",
    "        self.creValues = pd.read_csv(os.path.join(self.resourceDir, 'CREVALUES.csv'), index_col=0)\n",
    "        self.creTables = pd.read_csv(os.path.join(self.resourceDir, 'CRETABLES.csv'), index_col=0)\n",
    "        self.creSegs = pd.read_csv(os.path.join(self.resourceDir, 'CRESEGS.csv'), index_col=0)\n",
    "        self.creLocs = pd.read_csv(os.path.join(self.resourceDir, 'CRELOC.csv'), index_col=0)\n",
    "    \n",
    "    def make_names(self):\n",
    "        for itm in ['ITEM', 'SPELL', 'EFFECT']:\n",
    "            f = pd.read_csv(os.path.join(self.resourceDir, itm + '.csv'), index_col=0)\n",
    "            setattr(self, itm+'CODES', IDS.make_names(itm+'CODES', f))\n",
    "            setattr(self, itm, IDS.make_enum(itm, f, nameCol=-1, valueCol=-2))\n",
    "            if itm == 'ITEM':\n",
    "                setattr(self, itm+'CONTAINER', IDS.make_enum(itm+'CONTAINER', f[['ItemSlot', 'Item']]))\n",
    "            #setattr(self, itm, IDS.make_enum(itm, IDS.clean_duplicates(f), nameCol=-1, valueCol=-2))\n",
    "        for itm in ['WEAPON', *self.creTables['RefTable'].drop_duplicates()]: #Race, Kit, Class, Gender, Alignment\n",
    "            setattr(self, itm, IDS.make_enum(itm, pd.read_csv(os.path.join(self.resourceDir, itm + '.csv'), index_col=0)))\n",
    "        for itm in ['PARTYSLOTS', 'NPCSLOTS']:\n",
    "            f = pd.read_csv(os.path.join(self.resourceDir, itm + '.csv'), index_col=0)\n",
    "            f = f.groupby(f.columns[0]).apply(lambda x: x.index.to_list()).to_frame().reset_index()\n",
    "            setattr(self, itm, IDS.make_enum(itm, f, nameCol=0))\n",
    "               \n",
    "            \n",
    "    def pack(self, filename:str=None):\n",
    "        if filename is None:\n",
    "            filename = self.modified\n",
    "        self.zipfile.close()\n",
    "        with zipfile.ZipFile(filename, 'w') as target:\n",
    "            target.writestr(self.filelist[0], self.files[0], compress_type=zipfile.ZIP_STORED)\n",
    "            target.writestr(self.filelist[1], self.GAM.pack(), compress_type=zipfile.ZIP_STORED)\n",
    "            for _name, _f in zip(self.filelist[2:], self.files[2:]):\n",
    "                target.writestr(_name, _f, compress_type=zipfile.ZIP_STORED)\n",
    "                \n",
    "                \n",
    "    def optimize(self):\n",
    "        [_.set_value(_.optimize) for _ in self.GAM.VALUES]\n",
    "        [_.set_value(_.optimize) for c in self.Party for _ in c.VALUES]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Sav('save2.bg2save', 'BGEE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = s.Party[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEVEL</th>\n",
       "      <th>BASECOUNT</th>\n",
       "      <th>EFFCOUNT</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>INDEX</th>\n",
       "      <th>COUNTMEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x08\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x08\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x06\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\\x02\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x0e\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x06\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'\\x03\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\t\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x14\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x04\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'\\x04\\x00'</td>\n",
       "      <td>b'\\x08\\x00'</td>\n",
       "      <td>b'\\x08\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x18\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x02\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'\\x05\\x00'</td>\n",
       "      <td>b'\\x06\\x00'</td>\n",
       "      <td>b'\\x06\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x1a\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x02\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'\\x06\\x00'</td>\n",
       "      <td>b'\\x06\\x00'</td>\n",
       "      <td>b'\\x06\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'\\x02\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b'\\x03\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b'\\x04\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b'\\x05\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b'\\x06\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b'\\x07\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b'\\x08\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x01\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b'\\x00\\x00'</td>\n",
       "      <td>b'\\x11\\x00'</td>\n",
       "      <td>b'\\x11\\x00'</td>\n",
       "      <td>b'\\x02\\x00'</td>\n",
       "      <td>b'\\x1c\\x00\\x00\\x00'</td>\n",
       "      <td>b'\\x11\\x00\\x00\\x00'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LEVEL    BASECOUNT     EFFCOUNT         TYPE                INDEX  \\\n",
       "0   b'\\x00\\x00'    b'\\t\\x00'    b'\\t\\x00'  b'\\x00\\x00'  b'\\x00\\x00\\x00\\x00'   \n",
       "1   b'\\x01\\x00'    b'\\t\\x00'    b'\\t\\x00'  b'\\x00\\x00'  b'\\x08\\x00\\x00\\x00'   \n",
       "2   b'\\x02\\x00'    b'\\t\\x00'    b'\\t\\x00'  b'\\x00\\x00'  b'\\x0e\\x00\\x00\\x00'   \n",
       "3   b'\\x03\\x00'    b'\\t\\x00'    b'\\t\\x00'  b'\\x00\\x00'  b'\\x14\\x00\\x00\\x00'   \n",
       "4   b'\\x04\\x00'  b'\\x08\\x00'  b'\\x08\\x00'  b'\\x00\\x00'  b'\\x18\\x00\\x00\\x00'   \n",
       "5   b'\\x05\\x00'  b'\\x06\\x00'  b'\\x06\\x00'  b'\\x00\\x00'  b'\\x1a\\x00\\x00\\x00'   \n",
       "6   b'\\x06\\x00'  b'\\x06\\x00'  b'\\x06\\x00'  b'\\x00\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "7   b'\\x00\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "8   b'\\x01\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "9   b'\\x02\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "10  b'\\x03\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "11  b'\\x04\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "12  b'\\x05\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "13  b'\\x06\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "14  b'\\x07\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "15  b'\\x08\\x00'  b'\\x00\\x00'  b'\\x00\\x00'  b'\\x01\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "16  b'\\x00\\x00'  b'\\x11\\x00'  b'\\x11\\x00'  b'\\x02\\x00'  b'\\x1c\\x00\\x00\\x00'   \n",
       "\n",
       "               COUNTMEM  \n",
       "0   b'\\x08\\x00\\x00\\x00'  \n",
       "1   b'\\x06\\x00\\x00\\x00'  \n",
       "2   b'\\x06\\x00\\x00\\x00'  \n",
       "3   b'\\x04\\x00\\x00\\x00'  \n",
       "4   b'\\x02\\x00\\x00\\x00'  \n",
       "5   b'\\x02\\x00\\x00\\x00'  \n",
       "6   b'\\x00\\x00\\x00\\x00'  \n",
       "7   b'\\x00\\x00\\x00\\x00'  \n",
       "8   b'\\x00\\x00\\x00\\x00'  \n",
       "9   b'\\x00\\x00\\x00\\x00'  \n",
       "10  b'\\x00\\x00\\x00\\x00'  \n",
       "11  b'\\x00\\x00\\x00\\x00'  \n",
       "12  b'\\x00\\x00\\x00\\x00'  \n",
       "13  b'\\x00\\x00\\x00\\x00'  \n",
       "14  b'\\x00\\x00\\x00\\x00'  \n",
       "15  b'\\x00\\x00\\x00\\x00'  \n",
       "16  b'\\x11\\x00\\x00\\x00'  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.SpellMemorization.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x00\\x08\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'\\x00\\x00' + b'\\x08\\x00\\x00\\x00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "616803b2f19e1e9bcbf5ac8abb36b7fd12ef6319f5b436273684431f9aa2d3e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
